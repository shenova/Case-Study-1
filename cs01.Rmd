---
title: "CS01: Right-To-Carry "
author: "Donggyu Yu, Shenova Davis, Sihan Liang"
output: 
  html_document:
    toc: true
    toc_float: true
---

## Introduction

The misuse of firearms in public areas has led to the deaths of many innocent civilians, making it a critical social issue. This issue has sparked a controversial debate about whether to have federal-level laws prohibiting firearms in public areas. The debate is rooted in the Second Amendment of the United States Constitution, which guarantees the right to bear arms. Some argue that any attempt to restrict access to firearms is an infringement on that right. Others argue that gun violence is a public health crisis, and stricter gun control laws are necessary to keep people safe. This polarizing debate has been ongoing for decades and shows no signs of abating. 

Because there are no federal laws about carrying firearms in public, laws are enacted and enforced at the state level. However, laws vary by state, with some states being more or less strict than others. States like California, Hawaii, Maryland, Connecticut, Delaware, Massachusetts, New Jersey, New York, and Rhode Island have more strict laws on carrying weapons in public, requiring a permit authorized by an issuing authority.[^1] On the other hand, there are more states that do not require firearm permits, such as Alaska, Arizona, Kansas, Maine, Mississippi, Missouri, New Hampshire, Vermont, West Virginia, and Wyoming. [^1]

Given the varying levels of laws for carrying firearms, it is unclear whether carrying firearms or having more strict laws prevents serious incidents. Specifically, it is unknown whether having a Right to Carry law reduces or increases the violent crime rate. According to the Gun Violence Archive, as of March 4, 2023, there were 7,278 deaths and 5,358 injuries due to gun violence. [^2] Given these numbers and our interest in finding out the relationship between the Right to Carry law and violent crime rates, our study will help answer whether federal-level laws prohibiting firearms are needed.

Therefore, in this case study, we will analyze the relationship between the Right to Carry law and violent crime rates in the United States across years, starting from 1977-2010. Because there are different levels that differ by state, we will examine the difference of violence crime rates before and after the law was enacted. Also, we will dive deeper into what other aspects of relationships is available at the state level. By doing so, we hope to provide insights that can inform future policy decisions regarding firearms in public areas.


[^1]: "Concealed Carry States 2023", https://worldpopulationreview.com/state-rankings/concealed-carry-states
[^2]: "Gun Violence Archieve", https://www.gunviolencearchive.org/

## Question

1. What is the relationship between Right to Carry laws and violence rates in the US?
  b. How does the relationship differ between before and after the Right to Carry law was enacted.
2. What is the effect of multicollinearity on coefficient estimates from linear regression models when analyzing right to carry laws and violence rates?
3. Are some states violence crime rates influenced by other variables (e.g., Poverty Rate).

## The Data

The data we are going to use come from two studies with contradictory results. The first study is done by Donohue et al(2019) and the second study is done by Mustard and Lott(1996). Both datasets contain similar variables, including the year(1980 to 2010),State, unemployment rate,poverty rate, population, number of police staffing,violent crime count, the year in which the RTC law is enacted, and whether the RTC law is enacted in a given state in a given year. However, the two datasets differ in the demographic variables. The donohue dataset only accounts for males with age ranging from 15 to 39 years old whereas the Lott dataset accounts from both males and femals with age ranging from 10 to 65+ years old, which make the latter dataset more inclusive. However, the Donohue study suggests a positive relationship between violence crime rate and the RTC law, whereas the Lott study suggests a negative relationship. Therefore, in this study, we will first load the raw data and wrangle the data, and then conduct exploratory data analyses as well as modeling to account for this discrepancy and try to reveal unknown relationships and effects.

### Load packages

```{r load-packages, message=FALSE, warning=FALSE}
library(OCSdata) 
library(tidyverse)
library(pdftools)
library(readxl)
library(broom)
library(plm) 
library(car) 
library(rsample) 
library(GGally) 
library(ggcorrplot) 
library(ggrepel)
library(skimr)
library(tidymodels)
```

### Data Import

**Initial Import**

```{r}
# only get the data once
# remove comment when running for first time
#OCSdata::load_raw_data("ocs-bp-RTC-wrangling", outpath = '.')
```

**Importing Demographic & Population Data**

```{r, message=FALSE, warning=FALSE}
dem_77_79 <- read_csv("data/raw/Demographics/Decade_1970/pe-19.csv", skip = 5)

dem_80_89 <- list.files(recursive = TRUE,
                  path = "data/raw/Demographics/Decade_1980",
                  pattern = "*.csv",
                  full.names = TRUE) |> 
  purrr::map(~read_csv(., skip=5))

dem_90_99 <- list.files(recursive = TRUE,
                  path = "data/raw/Demographics/Decade_1990",
                  pattern = "*.txt",
                  full.names = TRUE) |> 
  map(~read_table2(., skip = 14))

dem_00_10 <- list.files(recursive = TRUE,
                  path = "data/raw/Demographics/Decade_2000",
                  pattern = "*.csv",
                   full.names = TRUE) |> 
   map(~read_csv(.))
```

The 70s data has different format and number of files compare to other decades. So we directly imported a single csv file for 70s data. 

For all other decades, since it has multiple files, we created using map at the end to iterate through each files that have same pattern (.csv for 80s and 00s, .txt for 90s), and store them in the list of files. We also used map calls for 80s, 90s and 00s to skip the rows in file that have unnecessary (i.e., headers) for all files in the list.

**Importing State FIPS Codes**

```{r}
STATE_FIPS <- readxl::read_xls("data/raw/State_FIPS_codes/state-geocodes-v2014.xls", skip = 5)
```

Imported State FIP codes stored as .xls using readxl.

**Importing Police Staffing Data**

```{r, message=FALSE, warning=FALSE}
ps_data <- read_csv("https://raw.githubusercontent.com/COGS137/datasets/main/pe_1960_2018.csv")
```

Since the police staffing data was not available in the OCS, we imported directly from the github and read in the csv file.

**Importing Unemployment Data**

```{r}
ue_rate_data <- list.files(recursive = TRUE,
                          path = "data/raw/Unemployment",
                          pattern = "*.xlsx",
                          full.names = TRUE) |> 
map(~read_xlsx(., skip = 10))

# Get names out of files (state names)
ue_rate_names <- list.files(recursive = TRUE,
                          path = "data/raw/Unemployment",
                          pattern = "*.xlsx",
                          full.names = TRUE) |>
map(~read_xlsx(., range = "B4:B6")) |>
  map(c(1,2)) |>
unlist()

names(ue_rate_data) <- ue_rate_names
```

Each unemployment contains headers that we do not need and are stored in different format. Each observations are years and each variables are months with annual summary at the end. We used similar approach as importing the demographic data, which is iterating through each file, removing headers, and store them as a list. Difference thogh is that we extracted the names of states using map call that is stored in "B4:B6" in the excel file and stored this inside a new variable `ue_rate_names`. We assigned the names of each list with its corresponding state using the `ue_rate_name`.

**Importing Poverty Data**

```{r, message=FALSE, warning=FALSE}
poverty_rate_data <- read_xls("data/raw/Poverty/hstpov21.xls", skip=2)
```
We read in a single excel file that contains poverty rate information. We skipped first two rows in file which contains the header.

**Importing Violent Crime Data**

```{r}
crime_data <- read_lines("data/raw/Crime/CrimeStatebyState.csv",
                         skip = 2, 
                         skip_empty_rows = TRUE)
```

Crime data also contains header and we are using read_lines from the readr package instead of read_csv because there are spaces and "\" in the column names.

**Importing Right-To-Carry Data**

```{r}
DAWpaper <- pdf_text("data/raw/w23510.pdf")
```

Right-To-Carry data are contained in a single pdf file which is odd compare to csv or xls. 

**Saving Data to Load for later work**

```{r}
save(dem_77_79, dem_80_89, dem_90_99, dem_00_10, 
     STATE_FIPS, 
     ps_data, 
     ue_rate_data, 
     poverty_rate_data,
     crime_data,
     DAWpaper, file = "data/imported_data_rtc.rda")
```

We are going to save all the imported data in a single rda file so we could load them when working on wrangling to avoid importing everytime we run the code.

### Data Wrangling

**Load Data to wrangle**

```{r}
load("data/imported_data_rtc.rda")
```

For the whole data wrangling process, we ultimately want to have `YEAR` and `STATE` using the state fip codes and year given in each file. Will orgaznie them by these two variables then will combine all of the other variables.

**Wrangling State FIPS Codes**

```{r}
STATE_FIPS <- STATE_FIPS |>
  rename(STATEFP = `State\n(FIPS)`,
         STATE = Name) |>
  select(STATEFP, STATE) |>
  filter(STATEFP != "00")
```

We are only going to store `STATEFP` which is the code for every state and `STATE` the contains name of each state. We are removing states that has "00" as code since they are not state.

**Wrangling Unemployment**

```{r}
ue_rate_data <- ue_rate_data |>
  #map means do something over and over again
  #map_df means you want it to be dataframe(tibble) in the end
  #Take State from each table and add (name of the list using .id)
  map_df(bind_rows, .id = "STATE") |>
  select(STATE, Year, Annual) |>
  rename("YEAR" = Year,
         "VALUE" = Annual) |>
  mutate(VARIABLE = "Unemployment_rate")
```

We are going to wrangle the unemployment data by combining rows with same state and ultimately combine the whole rows of those states. Then we are gonna select the variables we want which are `STATE`, `Year` and `Annual`. And change variable names accordingly to `YEAR`, `ANNUAL` and make a new column that assign each row to have its variable name (Unemployment_rate) under a `VARIABLE` column. The format of this `Population Data` will be the default format across all dataset that we are going to wrangle.

**Wrangling Demographic Data**

-<font size="3"> Demographic from 1977-1979 </font>

```{r}
dem_77_79 <- dem_77_79 |>
  rename("race_sex" =`Race/Sex Indicator`) |>
  mutate(SEX = str_extract(race_sex, "male|female"),
        RACE = str_extract(race_sex, "Black|White|Other"))|>
  select(-`FIPS State Code`, -`race_sex`) |>
  rename("YEAR" = `Year of Estimate`,
        "STATE" = `State Name`) |>
  filter(YEAR %in% 1977:1979)

dem_77_79 <- dem_77_79 |>
  pivot_longer(cols=contains("years"),
               names_to = "AGE_GROUP",
               values_to = "SUB_POP")
```

Since we are going to want our data format to be equal to the population data. We are first going to rename `Race/Sex Indicator` to `race_sex` to make it more informative. We are also going to extract information about race and sex under the renamed column `race_sex`. We see that it contains both sex and race information (e.g., "White male"), so we are going to store sex information from this string to `SEX` and race information to `RACE` 

Then we ar going to remove `FIPS State Code` and `race_sex` to rename the column `Year of Estimate` and `State Name` to `YEAR` and `STATE` (this will match the default format). Since we are interested in data after 1977, we are going to only extract `YEAR` that are between 1977 and 1979. 

Using pivot_longer that allow us to change the dataframe from wide to long with `YEAR`, `STATE` and other variables we are going to select all of the columns that contains "years" and going to change that and store them in the `AGE_GROUP`. The values that correspond to that `AGE_GROUP` will go into the `SUB_POP`.

-<font size="3"> Demographic from 1980s </font>

```{r}
dem_80_89 <- dem_80_89 |>
  map_df(bind_rows)

dem_80_89 <- dem_80_89 |>
  rename("race_sex" =`Race/Sex Indicator`) |>
  mutate(SEX = str_extract(race_sex, "male|female"),
        RACE = str_extract(race_sex, "Black|White|Other"))|>
  select( -`race_sex`) |>
  rename("YEAR" = `Year of Estimate`) |> 
  rename("STATEFP_temp" = "FIPS State and County Codes") |>
  mutate(STATEFP = str_sub(STATEFP_temp, start = 1, end = 2)) |>
    left_join(STATE_FIPS, by = "STATEFP") |>
  select(-STATEFP)

dem_80_89 <- dem_80_89 |>
  pivot_longer(cols=contains("years"),
               names_to = "AGE_GROUP",
               values_to = "SUB_POP_temp") |>
  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) |>
  summarize(SUB_POP = sum(SUB_POP_temp), .groups="drop")
```

The 80s data are in the list of different files, so first thing we do is combine all those files similar approach to wrangling 70s data. The wrangling for 80s will be slight different than the 77-79 data because we are working with lists of files. Will have same process of pivot longer as 70s data, but going to have different format than 70s. So we will store sub population temporarily (to `SUB_POP_temp`) because we are going to want to sum those from grouping by `YEAR`, `STATE`, `AGE_GROUP`, `SEX`, `RACE`.

-<font size="3"> Demographic from 1990s </font>

```{r}
dem_90_99 <- dem_90_99 |>
  map_df(bind_rows)

colnames(dem_90_99) <- c("YEAR", "STATEFP", "Age", "NH_W_M", "NH_W_F", "NH_B_M",
                         "NH_B_F", "NH_AIAN_M", "NH_AIAN_F", "NH_API_M", "NH_API_F",
                         "H_W_M", "H_W_F", "H_B_M", "H_B_F", "H_AIAN_M", "H_AIAN_F",
                         "H_API_M", "H_API_F")

dem_90_99 <- dem_90_99 |>
  drop_na() |>
  mutate(W_M = NH_W_M + H_W_M, W_F = NH_W_F + H_W_F,
         B_M = NH_B_M + H_B_M, B_F = NH_B_F + H_B_F,
         AIAN_M = NH_AIAN_M + H_AIAN_M, AIAN_F = NH_AIAN_F + H_AIAN_F,
         API_M = NH_API_M + H_API_M, API_F = NH_API_F + H_API_F) |>
  select(-starts_with("NH_"), -starts_with("H_"))

dem_90_99 <- dem_90_99 |>
  mutate(AGE_GROUP = cut(Age,
                         breaks = seq(0,90, by=5),
                         right = FALSE, labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP))) |>
  select(-Age) |>
  pivot_longer(cols = c(starts_with("W_"),
                        starts_with("B_"),
                        starts_with("AIAN_"),
                        starts_with("API_")),
               names_to = "RACE",
               values_to = "SUB_POP_temp") |>
  mutate(SEX = case_when(str_detect(RACE, "_M") ~ "Male",
                         TRUE ~ "Female"),
         RACE = case_when(str_detect(RACE, "W_") ~ "White",
                          str_detect(RACE, "B_") ~ "Black",
                          TRUE ~ "Other"))

dem_90_99 <- dem_90_99 |>
  left_join(STATE_FIPS, by = "STATEFP") |>
  select(-STATEFP) |>
  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) |>
  summarize(SUB_POP = sum(SUB_POP_temp), .groups="drop")
```

90s data are also stores in the list but in text file, so we will combine all rows to make it into one dataframe. 90s data have complete different variable names compared to raw 70s and 80s data so we are going determine what each variable represent and rename them in the format we want. There are 10 rows that has NA values across our data and each of those 10 rows only contains NA so we could drop them without affecting our dataset. After setting the names of variables we want to match the format of 70s and 80s data, so we add some variables in a new variables with same format of demographic and gender from 70s and 80s. Then we are going to do similar steps of pivot longer and store the variables that starts with "W", "B", "AIAN", "API in `RACE` and the corresponding values to `SUB_POP`. Then we are going to update the variables in `RACE` to match with 70s and 80s data, for example, "_M" to Male, "W_" to White.

-<font size="3"> Demographic from 2000s </font>

```{r}
dem_00_10 <- dem_00_10 |>
  map_df(bind_rows)

dem_00_10 <- dem_00_10 |>
  select(-ESTIMATESBASE2000,-CENSUS2010POP) |>
  filter(NAME != "United States",
         SEX != 0,
         RACE != 0,
         AGEGRP != 0, 
         ORIGIN == 0) |>
  select(-REGION, -DIVISION, -ORIGIN, -STATE) |>
  rename("STATE" = NAME,
         "AGE_GROUP" = AGEGRP)

dem_00_10 <- dem_00_10 |>
  mutate(SEX = factor(SEX, levels = 1:2, labels = c("Male", "Female")),
         RACE = factor(RACE, levels = 1:6, labels = c("White", "Black", rep("Other",4))),
         AGE_GROUP = factor(AGE_GROUP, levels = 1:18,
                            labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP)))

dem_00_10 <- dem_00_10 |>
  pivot_longer(cols=contains("ESTIMATE"), names_to = "YEAR", values_to = "SUB_POP_temp") |>
   mutate(YEAR = str_sub(YEAR, start=-4),
          YEAR = as.numeric(YEAR)) |> 
  group_by(YEAR, AGE_GROUP, STATE, SEX, RACE) |>
  summarize(SUB_POP = sum(SUB_POP_temp), .groups = "drop")
```
For 2000~2010 demographic data, we are first going to remove the variables that we do not need which are ESTIMATEBASE2000, and CENSUS2010POP, then we are going to filter out variables that has values of 0s and select the variables we want. The variables we want from this dataset is `NAME` and `AGEGRP`, to match the format, we will change its variable names to `STATE` and `AGE_GROUP`. To make it consistent, we will mutate our variables to extract gender and race information and going to factor them to be in levels and save them to variable `SEX`, `RACE` and `AGE_GROUP`. Then we will use pivot longer again to bring store all the population info to `SUB_POP` from `SUB_POP_temp`.

**Wrangling Population Data**

-<font size="3"> Population from 77-79 </font>
```{r}
pop_77_79 <- dem_77_79 |>
  group_by(YEAR, STATE) |>
  summarize(TOT_POP = sum(SUB_POP), .groups = "drop") 
```

-<font size="3"> Population from 80s </font>

```{r}
pop_80_89 <- dem_80_89 |>
  group_by(YEAR, STATE) |>
  summarize(TOT_POP = sum(SUB_POP), .groups = "drop") 
```

-<font size="3"> Population from 90s </font>

```{r}
pop_90_99 <- dem_90_99 |>
  group_by(YEAR, STATE) |>
  summarize(TOT_POP = sum(SUB_POP), .groups = "drop")
```

-<font size="3"> Population from 2000s </font>

```{r}
pop_00_10 <- dem_00_10 |>
  group_by(YEAR, STATE) |>
  summarize(TOT_POP = sum(SUB_POP), .groups = "drop")
```


For the all process of wrangling Population data, we will work with `SUB_POP` variable from demographics data. For each decade we will group by each `STATE` and `YEAR` and sum all the `SUB_POP` to calculate for total population and will save them in `TOT_POP`. 

**Combining Demographic and Population**

We are going to define a function since combining demographic and population data by decades will be repetitive steps.

```{r}
combine_demo_pop <- function(df_dem, df_pop){
  df_dem <- df_dem |>
  left_join(df_pop, by = c("YEAR", "STATE")) |>
  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>
  select(-SUB_POP, -TOT_POP) |>
  mutate(SEX = str_to_title(SEX))
}
```

Above we created function that takes in 2 parameters which we sill join using left join and calculate percentage of sub-population using the `TOT_POP` and `SUB_POP` times 100. After calculating the percentage of sub-population, we do not need `TOT_POP` and `SUB_POP` anymore so we will remove these variables.

-<font size="3"> Calling functions to combine </font>

```{r}
combined_df_77_79 <- combine_demo_pop(dem_77_79, pop_77_79)
combined_df_80_89 <- combine_demo_pop(dem_80_89, pop_80_89)
combined_df_90_99 <- combine_demo_pop(dem_90_99, pop_90_99)
combined_df_00_10 <- combine_demo_pop(dem_00_10, pop_00_10)
```

**Combining Demographic across decades**

Now that we have combined all of our demographic and population, we are going to combine them all to be in single dataframe.

```{r}
dem <- bind_rows(combined_df_77_79,
                 combined_df_80_89,
                 combined_df_90_99,
                 combined_df_00_10)
```
We see that it contains 187,272 observations and 6 variables.

**Wrangling Demographic data (Donohue)**

Using the combined data, we are going to make Demographic information for Donohue data. Donohue group have specific format and we want to match our wrangling to be in that format.

```{r}
DONOHUE_AGE_GROUPS <- c("15 to 19 years",
                        "20 to 24 years",
                        "25 to 29 years",
                        "30 to 34 years",
                        "35 to 39 years")

dem_DONOHUE <- dem |>
  filter(AGE_GROUP %in% DONOHUE_AGE_GROUPS,
               SEX == "Male") |>
  mutate(AGE_GROUP = fct_collapse(AGE_GROUP, "20 to 39 years"=c("20 to 24 years",
                                                                "25 to 29 years",
                                                                "30 to 34 years",
                                                                "35 to 39 years")),
         AGE_GROUP = str_replace_all(string = AGE_GROUP, 
                                     pattern = " ", 
                                     replacement = "_")) |>
  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) |>
  summarize(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = "drop") |>
  unite(col = "VARIABLE", RACE, SEX, AGE_GROUP, sep = "_") |>
  rename("VALUE" = PERC_SUB_POP)
```

We will first create a list containing the age group string to extract the `AGE_GROUP` since Donohue only wants age group between 15 to 39  and `SEX` that equals to "Male". We then will mutate the `AGE_GROUP` by using fct_collapse to include variables from the age group in 20~39 into 20~24, 25~29, 30~34 and 35~39. We will fill spaces between each variable in `AGE_GROUP` to "_". We will group by `YEAR`, `STATE`, `RACE`, `SEX`, `AGE_GROUP` to sum across all variables of `PERC_SUB_POP` within those variables as `PERC_SUB_POP`. Finally, we will rename the `PERC_SUB_POP` as `VALUE`. When we look at the glimpse of wrangled data we see the format of `YEAR`, `STATE`, `VARIABLE` representing gender and age group and `VALUE` representing the percentage of population in that gender and age group.

**Wrangling Demographic data (Lott)**

Using the combined data, we are going to make a Lott data.

```{r}
LOTT_AGE_GROUPS_NULL <- c("Under 5 years",
                          "5 to 9 years")

dem_LOTT <- dem |>
  filter(!(AGE_GROUP %in% LOTT_AGE_GROUPS_NULL) )|>
  mutate(AGE_GROUP = fct_collapse(AGE_GROUP,
                                  "10 to 19 years"=c("10 to 14 years", "15 to 19 years"),
                                  "20 to 29 years"=c("20 to 24 years", "25 to 29 years"),
                                  "30 to 39 years"=c("30 to 34 years", "35 to 39 years"),
                                  "40 to 49 years"=c("40 to 44 years", "45 to 49 years"),
                                  "50 to 64 years"=c("50 to 54 years", "55 to 59 years",
                                                     "60 to 64 years"),
                                  "65 years and over"=c("65 to 69 years", "70 to 74 years", 
                                                        "75 to 79 years", "80 to 84 years",
                                                        "85 years and over")),
         AGE_GROUP = str_replace_all(AGE_GROUP, " ", "_")) |>
  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) |>
  summarize(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = "drop") |>
  unite(col = "VARIABLE", RACE, SEX, AGE_GROUP, sep = "_") |>
  rename("VALUE" = PERC_SUB_POP)
```

Lott group has different format compared to Donohue group. Main difference is how they ranged the age group and separated them. So the whole process is same as Donohue but we will use different ranges of age. First we will make a list of age range as string that we will exclude which are "Under 5 years" and "5 to 9 years".After filtering out the age range, we will seperate the `AGE_GROUP` into format of Lott data. we then do same approach as Donohue data of getting `PERC_SUB_POP`. 

**Combine Population Data**

```{r}
population_data <- bind_rows(pop_77_79,
                             pop_80_89,
                             pop_90_99,
                             pop_00_10)

population_data <- population_data |>
  mutate(VARIABLE = "Population") |>
  rename("VALUE" = TOT_POP)

```

Using the wrangled population data stored as each decade, we will combine them to be in a single dataframe then will add `VARIABLE` to store "Population" to note that these are population data.

**Wrangling Police Staffing**

The Police staffing wrangling is already somewhat done and is in format of our interest.

However, we will need to do some wrangling and cleaning.

We want each state name to match other dataframes (e.g., AK -> Alaska) and remove states that we do not need.

```{r}
state_of_interest_NULL <- c("AS", "GM", "CZ", "FS", "MP", "OT", "PR", "VI")

ps_data <- ps_data |>
  filter(!(state_abbr %in% state_of_interest_NULL)) 
```

We will remove territories that are not states.

```{r}
state_abb_data <- tibble("state_abbr" = state.abb, "STATE" = state.name)
state_abb_data <- state_abb_data |>
  mutate(state_abbr = str_replace(string = state_abbr, 
                                  pattern = "NE", 
                                  replacement = "NB")) |>
  add_row(state_abbr = "DC", STATE = "District of Columbia")

ps_data <- ps_data |> 
  left_join(state_abb_data, by = "state_abbr") |>
  select(-state_abbr) |> 
  rename(YEAR = "data_year",
         VALUE = "officer_state_total") |>
  mutate(VARIABLE = "officer_state_total")
```

Then we will use state abbreviations to rename the states to match other dataframe as we stated earlier and have the new column that carries each value of "officer_state_total" in `VALUE` and `VARIABLE` will store  "officer_state_total" for further wrangling.

```{r}
denominator_temp <- population_data |> 
  select(-VARIABLE) |>
  rename("Population_temp"=VALUE) 

ps_data <- ps_data |> 
  left_join(denominator_temp, by=c("STATE","YEAR")) |>
  mutate(VALUE = (VALUE * 100000) / Population_temp) |>
  mutate(VARIABLE = "police_per_100k_lag") |>
  select(-Population_temp)
```

Since the we want to know the rate of police staffing across state in each year, we want to scale the police staffing to be per 100k. We used VALUE multiply by 100000 and divide `VALUE` from the `population_data` that has total population and changed the variable to be "police_per_100k_lag". The `VALUE` represents the rate of police staffing per 100000 population.

Now we see the `VALUE` calculated as per 100k.

**Wrangling Poverty Rate**

```{r}
colnames(poverty_rate_data) <- c("STATE", "Total", "Number", "Number_se",
                                 "Percent", "Percent_se")

poverty_rate_data <- poverty_rate_data |>
  filter(STATE != "STATE") |> 
  mutate(length_state = map_dbl(STATE, str_length)) |> # determine how long string in "STATE" column is
  filter(length_state < 100) |> # filter to only include possible state lengths
  mutate(STATE = str_replace(STATE, pattern = "D.C.", 
                              replacement = "District of Columbia" )) 

year_values <- poverty_rate_data |>
  filter(str_detect(STATE, "[:digit:]")) |>
  distinct(STATE)
year_values <- rep(pull(year_values, STATE), each = 52) # repeat values from STATE column 52 times each

poverty_rate_data <- poverty_rate_data |>
  mutate(year_value = year_values) |>
  select(-length_state) |>
  filter(str_detect(STATE, "[:alpha:]"))

poverty_rate_data <- poverty_rate_data |>
  filter(year_value != "2017") |> 
  filter(year_value != "2013 (18)") |>
  mutate(YEAR = str_sub(year_value, start = 1, end = 4)) |>
  select(-c(Number, Number_se, Percent_se, Total, year_value)) |>
  rename("VALUE" = Percent) |>
  mutate(VARIABLE = "Poverty_rate",
         YEAR = as.numeric(YEAR),
         VALUE = as.numeric(VALUE))
```

The raw poverty rate data did not have meaningful column names so we decided the column names and changed it for all columns. Then we will wrangle them to be in same format. After changing the column, we see some row contains the Column names throughout data, since we do not need them so we will filter out those rows. We will make new column that consists of length (in integer) of each state names and filter out the possible unnecessary state (length that are above 100 because of how data are stored in the file, there are comments). Then we will change "D.C" to "District of Columbia" to match the variable with other datasets. 

Then we are going to check `STATE` if it contains any digit because that is the row that contains the year information, then distinct them and store it to `year_values`. Then we will repeat and pull `year_values` and `STATE` 52 times because that is how many distinct states there are. We will store this pulled information in `year_values` (overwrite).

Now we are going to use `year_values` and store them in the new column `year_value`. We will remove the length state that we discussed above. Then we will check variables in `STATE` and just keep the states that only contains the letters.

Then we will filter out some years we do not need, and make a new column `YEAR` that contains first 4 letters of `year_value`. We will deselect columns we do not need then we will rename `Percent` to `VALUE` and this value is the poverty rate. Then we will update other columns we need to be in same data type and column name.

**Wrangling Crime Rate**

```{r}
crime_data <- crime_data[-((str_which(crime_data, "The figures shown in this column for the offense of rape were estimated using the legacy UCR definition of rape")-1): length(crime_data)+1)]

n_rows <- 2014-1977+1 # determine how many rows there are for each state
rep_cycle <- 4 + n_rows
rep_cycle_cut <- 2 + n_rows
colnames_crime <- (crime_data[4])

# specify which rows are to be deleted based on the file format
delete_rows <- c(seq(from = 2, 
                       to = length(crime_data),  
                       by = rep_cycle),
                 seq(from = 3, 
                       to = length(crime_data),
                       by = rep_cycle), 
                 seq(from = 4,
                       to = length(crime_data),
                       by = rep_cycle))
sort(delete_rows) # which rows are to be deleted
```
When we look at the raw crime rate data, we see that it is a mess, there are unnecessary summary information, and there are different tables. The first line of code will help us to clean a little bit of data by removing the rows starting from row that contains "The figures shown in this column for the offense of rape were estimated using the legacy UCR definition of rape" - 1 upto length of crime_data + 1. Then we will determine how many rows there are and store them in `n_rows`. We then assign how many repeat we need by adding 4 (unnecessary lines) and number of states (`n_rows`). Then we are going to figure out where to cut the cycle and is going to be 2 + n_rows (40). Then we will save column names of crime rates from row 4 and this 4th row contains colnames in string. Based on the file format, we found the rows that we do not need. Using rec_cycle we will save the rows to be deleted in 2nd, 3rd, and 4th row times rep_cycle (e.g., 2*rep_cycle, every 2nd row time rep_cycle) until length of the whole data. Then we will sort the rows to be deleted.

```{r}
# convince yourself you did it right
# should these rows be deleted?
crime_data[44:46]
```
We are checking here to see if we are removing right rows.

```{r, warning=FALSE}
crime_data <- crime_data[-delete_rows]

# extract state labels from data
state_labels <- crime_data[str_which(crime_data, "Estimated crime in ")]
state_labels <- str_remove(state_labels, pattern = "Estimated crime in ")
state_label_order <- rep(state_labels, each = n_rows) # repeat n_rows times

crime_data <- crime_data[-str_which(crime_data, "Estimated crime")]
crime_data_sep <- read_csv(I(crime_data), col_names = FALSE) |> 
  select(-X6) |># remove random extra-comma column
  drop_na()

# get column names for later
colnames(crime_data_sep) <- c("Year", 
                              "Population", 
                              "Violent_crime_total",
                              "Murder_and_nonnegligent_Manslaughter",
                              "Legacy_rape",
                              "Revised_rape", 
                              "Robbery",
                              "Aggravated_assault")
# add column names in
crime_data_sep <- bind_cols(STATE = state_label_order, crime_data_sep)

crime_data <- crime_data_sep |>
  mutate(VARIABLE = "Viol_crime_count") |>
  rename("VALUE" = Violent_crime_total) |>
  rename("YEAR" = Year) |>
  select(YEAR,STATE, VARIABLE, VALUE)
```

Now that we are sure which rows are to be deleted, we are going to deselect those rows. We then will extract state labels that are stored in each row that contains "Estimated crime in " and after extracting this string we will remove "Estimated crime in " since it is not needed anymore. It will allow us to only keep state names in `state_labels`. We will repeat this n_rows (40) and save each state name in `state_label_order`.

Now from a bit wrangled crime rate data, we will remove rows that will contain "Estimated crime".

We will import the raw data again to extract the column names, then we will bind columns using the same step we did with `state_label_order` to this separate data. Now from this separate data differs only by having column names compare to other data and we will just change column names to `YEAR` `VARIABLE` (store "Viol_crime_count"), VALUE and save it as `crime_data`.

**Wrangling RTC Laws**

```{r}
DAWpaper_p_62 <- DAWpaper[[62]]
str(DAWpaper_p_62, nchar.max = 1000) # see data
```
This file comes from PDF which is completely different type of file compare to other data. We looked through the pdf file and found that 62nd place contains the table we want.

```{r}
p_62 <- DAWpaper_p_62 |>
  str_split("\n") |>
  unlist() |>
  as_tibble() |>
  slice(-(1:2)) |> 
  rename(RTC = value) |>
  slice(-c(53:54)) |>  # physical page 60 marking; empty line removal
  mutate(RTC = str_replace_all(RTC, "\\s{40,}", "|N/A|"),
         RTC = str_trim(RTC, side = "left"),
         RTC = str_replace_all(RTC, "\\s{2,15}", "|"))
```

Here we will do wrangling for that 62nd place in pdf. We will first split each string by "\n" then we will store those string in to each column. Now the list contains the rows of infomration we need so we will store this in the tibble. We removed first and second row that contains the title of the table and an empty row. Since this dataframe is save in one column, we will rename that column to be `RTC`. We removed the page marking row and empty row. Then overwrite `RTC` to clean the whole string.

```{r, warning=FALSE}
p_62 <- pull(p_62, RTC) |>
  str_split( "\\|{1,}")  # split data on "|" symbol

# get the tibble!
p_62 <- as_tibble(do.call(rbind, p_62)) # rbind and not bind_cols here b/c we have no column names yet

colnames(p_62) <- c("STATE",
                    "E_Date_RTC",
                    "Frac_Yr_Eff_Yr_Pass",
                    "RTC_Date_SA")

p_62 <- p_62 |>
  slice(-c(1, 53:nrow(p_62))) # remove unnecessary rows

RTC <- p_62 |> 
  select(STATE, RTC_Date_SA) |>
  rename(RTC_LAW_YEAR = RTC_Date_SA) |>
  mutate(RTC_LAW_YEAR = as.numeric(RTC_LAW_YEAR)) |>
  mutate(RTC_LAW_YEAR = case_when(RTC_LAW_YEAR == 0 ~ Inf,
                                  TRUE ~ RTC_LAW_YEAR))
```

Since the data is now stored as a string separated by "|" we will pull all the variables in `RTC` and split and store them by "|". Using the splitted strings, we will create tibble that now looks like the data we want. But since we want to match the format of other data, we will rename some columns and remove unnecessary rows. We will then store the year information in `RTC_LAW_YEAR` by extracting string variable in `E_Date_RTC` and just extract the year then change it into numeric variable.

**Combining Donohue Date**

```{r}
# combine after all that wrangling!
DONOHUE_DF <- bind_rows(dem_DONOHUE,
                        ue_rate_data,
                        poverty_rate_data,
                        crime_data,
                        population_data,
                        ps_data)
```

Now that we wrangled all of our datasets, we will bind rows to make it into one single dataframe name `DONOHUE_DF` and note here we are using `dem_DONOHUE` because Lott and Donohue requires different format.

```{r}
# to wide format!
DONOHUE_DF <- DONOHUE_DF |>
  pivot_wider(names_from = "VARIABLE",
              values_from = "VALUE")

# add in RTC data!
DONOHUE_DF <- DONOHUE_DF |>
  left_join(RTC , by = c("STATE")) |>
  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,
                              TRUE ~ FALSE)) |>
 drop_na() # drop rows with missing information

# filter to only data where RTC laws were adopted between 1980-2010
# have crime data pre- and post-adoption this way
baseline_year <- min(DONOHUE_DF$YEAR)
censoring_year <- max(DONOHUE_DF$YEAR)

DONOHUE_DF <- DONOHUE_DF |>
  mutate(TIME_0 = baseline_year,
         TIME_INF = censoring_year) |>
  filter(RTC_LAW_YEAR > TIME_0)

# calculate violent crime rate; put population/crime on log scale
DONOHUE_DF <- DONOHUE_DF |>
  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,
         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),
         Population_log = log(Population))
```

After combining we will use pivot_wider to store them by variables in `VARIABLE` and values from `VALUE`. We will add wrangled RTC data into Donohue dataframe then will drop unnecessary rows. We will filter to extract the enacted year of RTC to have crime data pre and post of RTC law enacted. We will calculate the violent crime rate per 1k using the `Viol_crime_count`*1000/`population` and save it into new columns `Viol_crime_rate_1k`. Also created log of population since the raw population is too big.

**Combining Lott Data**

```{r}
LOTT_DF <- bind_rows(dem_LOTT,
                     ue_rate_data,
                     poverty_rate_data,
                     crime_data,
                     population_data,
                     ps_data) |>
  pivot_wider(names_from = "VARIABLE",
              values_from = "VALUE") |>
  left_join(RTC , by = c("STATE")) |>
  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,
                              TRUE ~ FALSE)) |>
   drop_na()

baseline_year <- min(LOTT_DF$YEAR)
censoring_year <- max(LOTT_DF$YEAR)

LOTT_DF <- LOTT_DF |>
  mutate(TIME_0 = baseline_year,
         TIME_INF = censoring_year) |>
  filter(RTC_LAW_YEAR > TIME_0)

LOTT_DF <- LOTT_DF |>
  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,
         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),
         Population_log = log(Population))
```

Same process as Donohue data but with `dem_LOTT` that has different structure compare to Donohue data.

**Saving the data**

```{r}
save(LOTT_DF, DONOHUE_DF, file = "data/wrangled_data_rtc.rda")
```

## Analysis

### Exploratory Data Analysis

First, we wanted to see the variables and information stored in each data set. First we started with LOTT_DF.

```{r}
glimpse(LOTT_DF)
skimr::skim(LOTT_DF)
```

Similarly, we did the same for the DONOHUE_DF data set. 

```{r}
glimpse(DONOHUE_DF)
skimr::skim(DONOHUE_DF)
```

To see the population trend for the data, we first looked at the overall population increase of all the entire population. This is the same analysis that we used in class. 

```{r}
DONOHUE_DF |>
  group_by(YEAR) |>
  summarise(Population = sum(Population)) |>
ggplot(aes(x = YEAR, y = Population)) +
  geom_line() +
  scale_x_continuous(
    breaks = seq(1980, 2010, by = 1),
    limits = c(1980, 2010),
    labels = c(seq(1980, 2010, by = 1))
  ) +
  labs(
    title = "Population has steadily increased",
    x = "Year",
    y = "Population"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90),
        plot.title.position = "plot")
```

Here we can see that the steadily population increases over time, which gives us insight for what to expect. Ther is a slight sharp increase in the 90s but we are unsure why. We can't find relationships using this graph to answer our question, but we will keep in mind that there is steady increase for population.

We then look at the overall crime rate over time. 

```{r}
df <- DONOHUE_DF |>
  group_by(YEAR) |>
  summarize(Viol_crime_count = sum(Viol_crime_count),
            Population = sum(Population),
            .groups = "drop") |>
  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population))

df |>
  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log)) +
  geom_line() +
  scale_x_continuous(
    breaks = seq(1980, 2010, by = 1),
    limits = c(1980, 2010),
    labels = c(seq(1980, 2010, by = 1))
  ) +
  scale_y_continuous(
    breaks = seq(5.75, 6.75, by = 0.25),
    limits = c(5.75, 6.75)
  ) +
  labs(
    title = "Crime rates fluctuate over time",
    x = "Year",
    y = "ln(violent crimes per 100,000 people)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90), 
        plot.title.position = "plot")
```

We can see here that the crime rate is not like the population rate and has changes in the data. The crime rate peaks at around 1991 and then decreases over time. Maybe there might be some special event around 90s. And since we are interested in before and after the Right to Carry Law enacted, we can try and see if the laws were enacted around 90s. [^3]

[^3]: "COGS137, Lecture12", https://cogs137.github.io/website/content/lectures/12-cs01-eda.html#cs01-right-to-carry-eda

We will observe above graph will all the states in our data

```{r}
p <- DONOHUE_DF |>
  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population)) |>
  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log, color = STATE)) +
  geom_point(size = 0.5) +
  geom_line(aes(group = STATE),
    size = 0.5,
    show.legend = FALSE
  ) +
  geom_text_repel(data = DONOHUE_DF |>
      mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population)) |>
      filter(YEAR == last(YEAR)),
      aes(label = STATE,x = YEAR, y = Viol_crime_rate_100k_log),
      size = 3, alpha = 1, nudge_x = 1, direction = "y",
      hjust = 1, vjust = 1, segment.size = 0.25, segment.alpha = 0.25,
      force = 1, max.iter = 9999) 

p + 
  guides(color = "none") +
  scale_x_continuous(
    breaks = seq(1980, 2015, by = 1),
    limits = c(1980, 2015),
    labels = c(seq(1980, 2010, by = 1), rep("", 5))
  ) +
  scale_y_continuous(
    breaks = seq(3.5, 8.5, by = 0.5),
    limits = c(3.5, 8.5)
  ) +
  labs(
    title = "States have different levels of crime",
    x = "Year", y = "ln(violent crimes per 100,000 people)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90), plot.title.position = "plot")
```

This graph is pretty complicated, however, we do see similar trends across states having District of Columbia as highest violent crime rate across years. There seems to be a peak around 90s and decrease after.

We also want to know if there is similar police trend since there might be more police staffing if there is more crime.

```{r}
DONOHUE_DF |>
  group_by(YEAR) |>
  summarise(Police = sum(police_per_100k_lag)) |> 
  ggplot(aes(x = YEAR, y = Police)) +
  geom_line() +
  scale_x_continuous(
    breaks = seq(1980, 2010, by = 1),
    limits = c(1980, 2010),
    labels = c(seq(1980, 2010, by = 1))
  ) +
  labs(
    title = "Police Presence has increased over time with fluctuations",
    x = "Year",
    y = "Police Presence per 100K people"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90),
        plot.title.position = "plot")
```

The overall trend for police staffin per 100k population is increasing. We can see steady increase from 1980 to 2000 but after 2000, there is no sharp increase anymore and stays steady.

We also looked at the crime rate by state to see which states had more crime, in relation to their population. To do this we used the Viol_crime_rate_1k variable, which gave us the crime count divided by the population. 

```{r}
ggplot(DONOHUE_DF, aes(x=YEAR)) + 
  geom_line(aes(y = Viol_crime_rate_1k, color = "Violent Crime Rate")) +
  
  labs(
    title="Violent Crime Rate by State and Year", 
    x="Year",
    y="Total Violent Crime Rate",
    color = "Color") +
  
  facet_wrap(~STATE, nrow = 5) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90), plot.title.position = "plot")
  
```

In this graph, we can see that the District of Columbia had the highest violent crime rate among the 50 states. And since District of Columbia has abnormal graph compared to other states, we can look at what factors caused this abnormality.

Since we are interested in District of Columbia we will just look at the tredn of District of columbia.
```{r}
DONOHUE_DF_DC <- DONOHUE_DF |>
  filter(STATE == "District of Columbia")
ggplot(DONOHUE_DF_DC, aes(x=YEAR)) + 
  geom_line(aes(y = Viol_crime_rate_1k, color = "Violent Crime Rate")) +
  geom_line(aes(y = Poverty_rate, color = "Poverty Rate")) + 
  geom_line(aes(y = Unemployment_rate, color = "Unemployment Rate"))+
  labs(
    title="Violent Crime Rate, Poverty rate, Unemployment Rate in District of Columbia", 
    x="Year",
    y=" ",
    color = "Color") +
  theme(axis.text.x = element_text(angle = 90), plot.title.position = "plot") +
  theme_minimal()

```

Here we see similar trend in Poverty rate and Violent crime rate and has the highest peak around 1995. Based on this observation, we can do statistical analysis later if this trend is just correlation or not.

Finally, we looked at the violent crime rate before and after the Right To Carry (RTC) law was passed in each state. States that have no red continuation line are states that have not passed the RTC law. We analyzed this graph to see if there was an overall increase or decrease in the violent crime rate after the RTC law was passed, or if there was any relationship between the law and the rate of violent crime. 

```{r}
ggplot(LOTT_DF, aes(x = YEAR, y = Viol_crime_rate_1k)) + 
  geom_line(aes(color = ifelse(YEAR < RTC_LAW_YEAR, "Before RTC", "After RTC"))) + 
  
  labs(
    title = "Trend of Violent Crime Rate Before and After RTC Law by State", 
    x = "Year",
    y = "Violent Crime Rate per 1000",
    color = "Before or After"
  ) +

  # geom_vline(aes(xintercept = RTC_LAW_YEAR), color = "red", linetype = "dashed") +
  
  facet_wrap(~STATE, nrow = 5)  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90), plot.title.position = "plot")
  

```

We can see the changes in the crime rate before and after Right to Carry law was enacted and some states do not have Right to Carry Law. And based on this observation, we can divide the dataframe into the states that does not have Right to Carry Law and observation

### Data Analysis

Data that we will be working with are Lott and Donohue data. These data are Panel data meaning it contains repeated measures of multiple panel members or individuals over period. Both of our data contains multiple units across multiple year. In our case the unit will be the states and the year will be the time points. Also our data is going to be a balanced data meaning that there are 44 states across 31 years which equals to have 1364 observations. [^4] In order to find relationship between the Right to Carry law and violent rate, we will be building Panel Linear Regression. Each predictor will be the variables from a state across different year. 

[^4]: "COGS137, Winter 2023 Lecture 14", https://cogs137.github.io/website/content/lectures/13-cs01-analysis.html#questions

#### Relationship between Right to Carry and Violence Rate

```{r}
d_panel_DONOHUE <- pdata.frame(DONOHUE_DF, index = c("STATE", "YEAR"))
```

We will store our `DONOHUE_DF` in `d_panel_DONOHUE` which is a data.frame class, with individual variable name being `STATE` and time variable to be `YEAR`.

```{r}
head(d_panel_DONOHUE, n = 3)
```

when we look at the data, we see that each row is now a combination of `STATE` and each time point `YEAR`.

Since we have seen in Exploratory data analysis that there are changes in violent crime over time within state, we are going to use two ways effect model to check if there are effects of `STATE` and `YEAR` on violent rate crime. Also, we will use within state model to see how each state varies over time. Although we can just include `RTC_LAW` to predict violent crime rate since that is our question, however, we will be including all other variables because there could be relationship between these variables and violent crime rate. This applies to both Donohue and Lott data.

**Panel Regression Mode for Donohue**

```{r}
DONOHUE_OUTPUT <- plm(Viol_crime_rate_1k_log ~
                      RTC_LAW +
                      White_Male_15_to_19_years +
                      White_Male_20_to_39_years +
                      Black_Male_15_to_19_years +
                      Black_Male_20_to_39_years +
                      Other_Male_15_to_19_years +
                      Other_Male_20_to_39_years +
                      Unemployment_rate +
                      Poverty_rate +
                      Population_log +
                      police_per_100k_lag,
                      effect = "twoways",
                      model = "within",
                      data = d_panel_DONOHUE)

DONOHUE_OUTPUT_TIDY <- tidy(DONOHUE_OUTPUT, conf.int = 0.95)
DONOHUE_OUTPUT_TIDY
DONOHUE_OUTPUT_TIDY$Analysis <- "DONOHUE"
```

Here is the result of our model with confidence interval being 95% for Donohue data.

**Panel Regression Mode for Lott**

```{r}
LOTT_variables <- LOTT_DF |>
  select(RTC_LAW,
         contains(c("White", "Black", "Other")),
         Unemployment_rate,
         Poverty_rate,
         Population_log,
         police_per_100k_lag) |>
  colnames()

LOTT_fmla <- as.formula(paste("Viol_crime_rate_1k_log ~", paste(LOTT_variables, collapse = " + ")))
```

We used the as.formula method to extract all variables names because they are really long. 

Then we used same approach of fitting panel regression model for Donohue to Lott data.

```{r}
d_panel_LOTT <- pdata.frame(LOTT_DF, index = c("STATE", "YEAR"))

LOTT_OUTPUT <- plm(LOTT_fmla,
                   model = "within",
                   effect = "twoways",
                   data = d_panel_LOTT)

LOTT_OUTPUT_TIDY <- tidy(LOTT_OUTPUT, conf.int = 0.95)
LOTT_OUTPUT_TIDY
LOTT_OUTPUT_TIDY$Analysis <- "LOTT"
```

Here is the result of our model with confidence interval being 95% for Lott data.

Since we are interested in the coefficient of `RTC_LAW` (baseline is TRUE), we will compare that coefficient from Donohue and Lott data

```{r}
comparing_analyses <- DONOHUE_OUTPUT_TIDY |>
  bind_rows(LOTT_OUTPUT_TIDY) |>
  filter(term == "RTC_LAWTRUE")

comparing_analyses
```

The estimate value from Donohue data is 0.024 which means it has a slightly positive slope. The negative slope indicates that when there is Right to Carry Law, the violence crime rate goes up slightly. 

The estimate value from Lott data is -0.0518 which means it has a slightly negative slope. The negative slope indicates that when there is Right to Carry Law, the violence crime rate goes down slightly. 

Based on how we structured our demographic factors and other variables (difference in Lott and Donohue) there is conflicting result of predicting violent crime rate based on whether there is Right to Carry law or not.

Here is the visualization for our better understaing of the model.

```{r}
ggplot(comparing_analyses) +
  geom_point(aes(x = Analysis, y = estimate)) +
  geom_errorbar(aes(x = Analysis, ymin = conf.low, ymax = conf.high), width = 0.25) +
  geom_hline(yintercept = 0, color = "red") +
  scale_y_continuous(
    breaks = seq(-0.2, 0.2, by = 0.05),
    labels = seq(-0.2, 0.2, by = 0.05),
    limits = c(-0.2, 0.2)
  ) +
  geom_segment(aes(x = 1, y = 0.125, xend = 1, yend = 0.175),
    arrow = arrow(angle = 45, ends = "last", type = "open"),
    size = 2, color = "green", lineend = "butt", linejoin = "mitre"
  ) +
  geom_segment(aes(x = 2, y = -0.125, xend = 2, yend = -0.175),
    arrow = arrow(angle = 45, ends = "last", type = "open"),
    size = 2, color = "red", lineend = "butt", linejoin = "mitre"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.text = element_text(size = 8, color = "black"),
    plot.title.position = "plot"
  ) +
  labs(
    title = "Effect estimate on ln(violent crimes per 100,000 people)",
    y = "  Effect estimate (95% CI)"
  )
```

This plot indicates that Violence rate increase for Donohue, and decrease for Lott. The gap between the each line (line above to line below) is the 95% confidence interval. Since the confidence interval in Donohue data cross 0, this means that sometimes there is going to be no effect but most of the time there is. The Lott confidence interval does not cross 0 which means it always going to have slight decrease if there is Right to Carry Law.

To answer how the Right to Carry Law affected the violence crime rate, we will pick one state that seemed to have decay in violence crime rate after the law was enacted from exploratory data analysis section.

Nevada show a decline in violent crime after the RTC law was enacted based on the graph we visualized in exploratory data analysis section.

#### Before and After Law enacted

To answer how the Right to Carry Law affected the violence crime rate, we will pick one state that seemed to have decay in violence crime rate after the law was enacted from exploratory data analysis section.

Nevada show a decline in violent crime after the RTC law was enacted based on the graph we visualized in exploratory data analysis section.

We will first divide up Nevada data into before and after the law was enacted.

```{r}
nevada_DONOHUE <- DONOHUE_DF |>
  filter(STATE == "Nevada")
nevada_LOTT <- LOTT_DF |>
  filter(STATE == "Nevada")
```

Here is our two data for nevada (before and after) the law enacted.

We will fit linear regression for both before and after the law enacted for Nevada.

```{r}
nevada_lin_mod_donohue <- lm(
  Viol_crime_rate_1k_log ~
    RTC_LAW +
    White_Male_15_to_19_years +
    White_Male_20_to_39_years +
    Black_Male_15_to_19_years +
    Black_Male_20_to_39_years +
    Other_Male_15_to_19_years +
    Other_Male_20_to_39_years +
    Unemployment_rate +
    Poverty_rate +
    Population_log +
    police_per_100k_lag,
  data = nevada_DONOHUE
)
nevada_lin_mod_donohue 
```

```{r}
LOTT_fmla <- as.formula(paste("Viol_crime_rate_1k_log ~", paste(LOTT_variables, collapse = " + ")))

nevada_lin_mod_lott <- lm(LOTT_fmla,
                          data = nevada_LOTT)
nevada_lin_mod_lott 
```

Since that it seems like there is issue of mulicolinearity in Lott data, we will determine our answer using just Donohue data to compare before and after the  Right to carry law enacted.

```{r}
nevada_lin_mod_donohue <- tidy(nevada_lin_mod_donohue)
nevada_result<-nevada_lin_mod_donohue |>
  filter(term == "RTC_LAWTRUE" | term == "(Intercept)")
nevada_result
nevada_lin_mod_donohue
```
The coefficient estimate tells us that before the Law was enacted for Nevada (i.e., RTC_LAW is FALSE), predicted violence crime rate is 35.42. And after the law was enacted, the predicted violence crime rate is going to be (35-0.199898). This tells us that after the Right to Carry law for Nevada, there is decrease of violence crime rate.

#### Solving Multicolinearity

**Thinking about Multicolinearity**

Now we have to consider the multicolinearity since we have a lot of predictors which can result high relationships between each other. If there is strong relationships between predictors, it is going to cause a problem to the model. Since linear model predicts if one independent variable's unit change the dependent variable will change, however if there is colinearity, that unit change will not satisfy our results.

We will be looking at what variables have potential to cause multicolinearity

**Diagnosing Multicollinearity**

We will first look at pair plot for non-demographic data (violent rate, unemployment rate, poverty rate, and log population).

```{r}
DONOHUE_DF |>
  select(RTC_LAW,
         Viol_crime_rate_1k_log,
         Unemployment_rate,
         Poverty_rate,
         Population_log) |>
  ggpairs(columns = c(2:5),
          lower = list(continuous = wrap("smooth_loess",
                                         color = "red",
                                         alpha = 0.5,
                                         size = 0.1)))
```

Above plot shows the correlations by each variable, and we see that there are some correlation between poverty rate and unemployment rate, and log population and violent crime rate. However, the correlation seems to be not strong so we do not have to worry about multicolinearity for non-demographic variables.

We then will see correlation plot and heatmap that compares demographic data (Donohue).

```{r}
cor_DONOHUE_dem <- cor(DONOHUE_DF |> select(contains("_years")))

ggcorrplot(cor_DONOHUE_dem,
  tl.cex = 6,
  hc.order = TRUE,
  colors = c(
    "red",
    "white",
    "red"
  ),
  outline.color = "transparent",
  title = "Correlation Matrix: Donohue",
  legend.title = expression(rho)
)
```

Here we can observe that there are heavy correlation within race and race shows more correlation than the age.

This plot is the heatmap that compares demographic data for Lott data.

```{r}
cor_LOTT_dem <- cor(LOTT_DF |> select(contains("_years")))

corr_mat_LOTT <- ggcorrplot(cor_LOTT_dem,
  tl.cex = 6,
  hc.order = TRUE,
  colors = c(
    "red",
    "white",
    "red"
  ),
  outline.color = "transparent",
  title = "Correlation Matrix: Lott",
  legend.title = expression(rho)
)

corr_mat_LOTT
```

When we look at this plot, it has similar symptom as Donohue data. It has heavy correlation within race and smaller correlation within age.

Now that we see multicolinearity among ethnicity, we can re-sample our data (i.e., remove one observation and check the change in estimate). We do this multiple times and save the coefficient for each trial. And this procedure is known as Leave One Out Cross Validation.

**Leave One Out Cross Validation for Donohue**

First we will be splitting our data.

```{r}
## split data
set.seed(124)
d_panel_DONOHUE <- pdata.frame(DONOHUE_DF, index = c("STATE", "YEAR"))
DONOHUE_splits <- d_panel_DONOHUE |> loo_cv()
```

This procedure tells that we will leave 1 obseravtion (different observation in each trial) out for 1364 trials.

Then we will use map to train our splitted Donohue data.

```{r}
DONOHUE_subsets <- map(pull(DONOHUE_splits, splits), training)
glimpse(DONOHUE_subsets[[1]])
```

Here we are looking at the first trial from the splitted data, everything is same except one observation is removed at random.

Now we are going to run the panel regression model on each of the subset in the splitted data.
And since it will be repetitive step we will define a function for bootstrapping.

```{r}
fit_nls_on_bootstrap_DONOHUE <- function(subset) {
  plm(Viol_crime_rate_1k_log ~ RTC_LAW +
        White_Male_15_to_19_years +
        White_Male_20_to_39_years +
        Black_Male_15_to_19_years +
        Black_Male_20_to_39_years +
        Other_Male_15_to_19_years +
        Other_Male_20_to_39_years +
        Unemployment_rate +
        Poverty_rate +
        Population_log +
        police_per_100k_lag,
      data = data.frame(subset),
      index = c("STATE", "YEAR"),
      model = "within",
      effect = "twoways")
}
```

We now will apply our subsets in the function

NOTE: This code runs long so remind to cashe them

```{r, eval=FALSE}
subsets_models_DONOHUE <- map(DONOHUE_subsets, fit_nls_on_bootstrap_DONOHUE)

subsets_models_DONOHUE <- subsets_models_DONOHUE |>
  map(tidy)

subsets_models_DONOHUE[[1]]
```

Here we see the model of the first subset and the whole subset model will contain 1364 different models having one observation left out for each trial. The coefficients should not change by removing one observation, so if there is some change in coefficient, that means there is something going on with our model.

Saving the output and loading  model Donohue.

```{r, eval= FALSE}
save(subsets_models_DONOHUE,
  file = "data/DONOHUE_simulations.rda")
```

```{r}
load("data/DONOHUE_simulations.rda")
```

We are going to do the same process for Lott data.

```{r}
set.seed(124)
d_panel_LOTT <- pdata.frame(LOTT_DF, index = c("STATE", "YEAR"))
LOTT_splits <- d_panel_LOTT |> loo_cv()
LOTT_subsets <- map(pull(LOTT_splits, splits), training)
```

Defining a function for bootstrapping.

```{r}
fit_nls_on_bootstrap_LOTT <- function(split) {
  plm(LOTT_fmla,
      data = data.frame(split),
      index = c("STATE", "YEAR"),
      model = "within",
      effect = "twoways"
  )
}
```

We will resuse the code from before since there are a lot of variabled to work with for Lott data.

```{r}
LOTT_variables <- LOTT_DF |>
  select(RTC_LAW,
         contains(c("White", "Black", "Other")),
         Unemployment_rate,
         Poverty_rate,
         Population_log,
         police_per_100k_lag) |>
  colnames()
LOTT_fmla <- as.formula(paste("Viol_crime_rate_1k_log ~", paste(LOTT_variables, collapse = " + ")))
```

Using the defined function

NOTE: This code runs long so remind to cashe them

```{r, eval=FALSE}
subsets_models_LOTT <- map(LOTT_subsets, fit_nls_on_bootstrap_LOTT)
subsets_models_LOTT <- subsets_models_LOTT |>
  map(tidy)
```

Saving the output model for Lott.

```{r, eval= FALSE}
save(subsets_models_LOTT,
  file = "data/LOTT_simulations.rda")
```


```{r}
load("data/LOTT_simulations.rda")
```

We now will clean up our result and visualize them by plotting them 

```{r}
names(subsets_models_DONOHUE) <- paste0("DONOHUE_", seq_len(length(subsets_models_DONOHUE)))

names(subsets_models_LOTT) <-
  paste0("LOTT_", 1:length(subsets_models_LOTT))
```

We will take the result from both Donohue and Lott subset and save them in a single dataframe.

```{r}
simulations_DONOHUE <- subsets_models_DONOHUE |>
  bind_rows(.id = "ID") |>
  mutate(Analysis = "Donohue")

simulations_LOTT <- subsets_models_LOTT |>
  bind_rows(.id = "ID") |>
  mutate(Analysis = "Lott")

simulations <- bind_rows(simulations_DONOHUE, simulations_LOTT)

# order for easier comparison
simulations <- simulations |>
  mutate(term = factor(term,
    levels = c(
      str_subset(unique(pull(simulations, term)), "years", negate = TRUE),
      sort(str_subset(unique(pull(simulations, term)), "years")))))
```

Here we cleaned the dataset because of the plot that we will generate.

Here is the visualization of the result.

```{r}
simulations |>
  ggplot(aes(x = term, y = estimate)) +
  geom_boxplot() +
  facet_grid(. ~ Analysis, scale = "free_x", space = "free", drop = TRUE) +
  labs(title = "Coefficient estimates",
       subtitle = "Estimates across leave-one-out analyses",
       x = "Term",
       y = "Coefficient",
       caption = "Results from simulations") +
  theme_linedraw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 70, hjust = 1),
        strip.text.x = element_text(size = 14, face = "bold"),
        plot.title.position="plot")
```

When we look at this visualization, we see a lot going on in Lott data with demographic groups of "Others"

Now we are going to look at standard deviation, meaning how much variance is around that estimate (coefficient).

```{r}
coeff_sd <- simulations |>
  group_by(Analysis, term) |>
  summarize("SD" = sd(estimate))

coeff_sd |>
  ggplot(aes(x = Analysis, y = SD)) +
  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +
  labs(title = "Coefficient variability",
       subtitle = "SDs of coefficient estimates from leave-one-out analysis",
       x = "Term",
       y = "Coefficient Estimate \n Standard Deviations",
       caption = "Results from simulations") +
  theme_minimal() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(size = 8, color = "black"),
        axis.text.y = element_text(color = "black"),
        plot.title.position="plot")
```
Each points represents the coefficient estimate standard deviations and here we are observing the coefficient variability from leave-one-out analysis. Based on the graph, we see that Donohue data is more stable than the Lott data. And Lott data has a lot of variables has high standard deviation, meaning larger variance.

Since there is high variance in Lott analysis, we are going to calculate variance inflation factor meaning we will check the index of how much the variance of coefficient is increased due to collinearity. If the variance inflaction factor is high, that means there is collinearity involved in the variable.

**Calculating Variance Inflation Factor for Donohue**

```{r}
DONOHUE_OUTPUT <- plm(Viol_crime_rate_1k_log ~
                      RTC_LAW +
                      White_Male_15_to_19_years +
                      White_Male_20_to_39_years +
                      Black_Male_15_to_19_years +
                      Black_Male_20_to_39_years +
                      Other_Male_15_to_19_years +
                      Other_Male_20_to_39_years +
                      Unemployment_rate +
                      Poverty_rate +
                      Population_log +
                      police_per_100k_lag,
                      effect = "twoways",
                      model = "within",
                      data = d_panel_DONOHUE)

# create model matrix
lm_DONOHUE_data <- as.data.frame(model.matrix(DONOHUE_OUTPUT))

# define model
lm_DONOHUE_data <- lm_DONOHUE_data |> 
  mutate(Viol_crime_rate_1k_log = plm::Within(pull(
    d_panel_DONOHUE, Viol_crime_rate_1k_log
  )), effect = "twoways")

# specify model
lm_DONOHUE <- lm(Viol_crime_rate_1k_log ~
RTC_LAWTRUE +
  White_Male_15_to_19_years +
  White_Male_20_to_39_years +
  Black_Male_15_to_19_years +
  Black_Male_20_to_39_years +
  Other_Male_15_to_19_years +
  Other_Male_20_to_39_years +
  Unemployment_rate +
  Poverty_rate +
  Population_log +
  police_per_100k_lag,
data = lm_DONOHUE_data
)

# calculate VIF
vif_DONOHUE <- vif(lm_DONOHUE)
```

We calculated the variance inflation factor for whole analysis for Donohue data.

```{r}
# combine into nice table
vif_DONOHUE <- vif_DONOHUE |>
  as_tibble() |>
  cbind(names(vif_DONOHUE)) |>
  as_tibble()
colnames(vif_DONOHUE) <- c("VIF", "Variable")

vif_DONOHUE |>
  arrange(desc(VIF))
```
Here is the indexes of variation inflation factor for Donohue analysis.

**Calculating Variance Inflation Factor for Lott**

```{r}
LOTT_OUTPUT <- plm(LOTT_fmla,
                   model = "within",
                   effect = "twoways",
                   data = d_panel_LOTT)

lm_LOTT_data <- as.data.frame(model.matrix(LOTT_OUTPUT))
lm_LOTT_data <- lm_LOTT_data |>
  mutate(Viol_crime_rate_1k_log = plm::Within(pull(d_panel_LOTT, Viol_crime_rate_1k_log), effect = "twoways")) |>
  rename(RTC_LAW = RTC_LAWTRUE)
lm_LOTT <- lm(LOTT_fmla, data = lm_LOTT_data)

vif_LOTT <- vif(lm_LOTT)
vif_LOTT <- vif_LOTT |>
  as_tibble() |>
  cbind(names(vif_LOTT)) |>
  as_tibble()
colnames(vif_LOTT) <- c("VIF", "Variable")
```

We calculated the variance inflation factor for whole analysis for Lott data.

```{r}
# clean up names
vif_LOTT |> 
  mutate(Variable = str_replace(string = Variable,
                                pattern = "RTC_LAW",
                                replacement = "RTC_LAWTRUE")) |>
  arrange(desc(VIF))
```

Here is the indexes of variation inflation factor for Donohue analysis.

Let's now look at the result across analyses

```{r}
vif_DONOHUE <- vif_DONOHUE |>
  mutate(Analysis = "Donohue")
vif_LOTT <- vif_LOTT |>
  mutate(Analysis = "Lott")
vif_df <- bind_rows(vif_DONOHUE, vif_LOTT)
head(vif_df)
```

Here is the visualization of our variance inflation factor from Donohue and Lott for us to determine where the collinearity exists.

```{r}
vif_df |>
  ggplot(aes(x = Analysis, y = VIF)) +
  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +
  geom_hline(yintercept = 10, color = "red") +
  geom_text(aes(.75, 13, label = "typical cutoff of 10")) +
  coord_trans(y = "log10") +
  labs(title = "Variance inflation factors") +
  theme_minimal() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(color = "black"),
        axis.text.y = element_text(color = "black"))
```

This graph tells us the Donohue data is safe from the colinearity, however, our Lott data seems to have some problem. To fix this issue, we will have to decide what to do.

**Correct for MULTICOLINEARITY** 

We will try different parameterization of predictors on Lott data.

```{r}

LOTT_DF1 = LOTT_DF%>%
  mutate(black_female_20_39 = Black_Female_20_to_29_years+Black_Female_30_to_39_years,
         black_female_40plus = Black_Female_40_to_49_years+Black_Female_50_to_64_years+Black_Female_65_years_and_over,
         black_male_20_39 = Black_Male_20_to_29_years+Black_Male_30_to_39_years,
         black_male_40plus = Black_Male_40_to_49_years+Black_Male_50_to_64_years+Black_Male_65_years_and_over,
         white_female_20_39 = White_Female_20_to_29_years+White_Female_30_to_39_years,
         white_female_40plus = White_Female_40_to_49_years+White_Female_50_to_64_years+White_Female_65_years_and_over,
         white_male_20_39 = White_Male_20_to_29_years+White_Male_30_to_39_years,
         white_male_40plus = White_Male_40_to_49_years+White_Male_50_to_64_years+White_Male_65_years_and_over,
         other_female_20_39 = Other_Female_20_to_29_years+Other_Female_30_to_39_years,
         other_female_40plus = Other_Female_40_to_49_years+Other_Female_50_to_64_years+Other_Female_65_years_and_over,
         other_male_20_39 = Other_Male_20_to_29_years+Other_Male_30_to_39_years,
         other_male_40plus = Other_Male_40_to_49_years+Other_Male_50_to_64_years+Other_Male_65_years_and_over
  )

d_panel_LOTT1 <- pdata.frame(LOTT_DF1, index = c("STATE", "YEAR"))

```

```{r}

LOTT_OUTPUT_modified <- plm(Viol_crime_rate_1k_log ~
                              RTC_LAW +
                              White_Male_10_to_19_years +
                              white_male_20_39 +
                              white_male_40plus+
                              Black_Male_10_to_19_years +
                              black_male_20_39 +
                              black_male_40plus+
                              other_male_40plus+
                              Other_Male_10_to_19_years +
                              other_male_20_39 +
                              Unemployment_rate +
                              Poverty_rate +
                              Population_log +
                              police_per_100k_lag,
                            effect = "twoways",
                            model = "within",
                            data = d_panel_LOTT1)

LOTT_OUTPUT_modified_TIDY <- tidy(LOTT_OUTPUT_modified, conf.int = 0.95)
LOTT_OUTPUT_modified_TIDY
LOTT_OUTPUT_modified_TIDY$Analysis <- "LOTT"


```


```{r}
# create model matrix
lm_LOTT_data1 <- as.data.frame(model.matrix(LOTT_OUTPUT_modified))

# define model
lm_LOTT_data1 <- lm_LOTT_data1 |> 
  mutate(Viol_crime_rate_1k_log = plm::Within(pull(
    d_panel_LOTT1, Viol_crime_rate_1k_log
  )), effect = "twoways")

# specify model
lm_LOTT1 <- lm(Viol_crime_rate_1k_log ~RTC_LAWTRUE +
                 White_Male_10_to_19_years +
                 white_male_20_39 +
                 white_male_40plus+
                 Black_Male_10_to_19_years +
                 black_male_20_39 +
                 black_male_40plus+
                 other_male_40plus+
                 Other_Male_10_to_19_years +
                 other_male_20_39 +
                 Unemployment_rate +
                 Poverty_rate +
                 Population_log +
                 police_per_100k_lag,
               data = lm_LOTT_data1
)

# calculate VIF
vif_LOTT1 <- vif(lm_LOTT1)
```

We calculated the variance inflation factor for whole analysis for the new data.

```{r}
# combine into nice table
vif_LOTT1 <- vif_LOTT1 |>
  as_tibble() |>
  cbind(names(vif_LOTT1)) |>
  as_tibble()
colnames(vif_LOTT1) <- c("VIF", "Variable")

vif_LOTT1 |>
  arrange(desc(VIF))
```

Based on the results, we can see that the VIF values are much more lower for this newly modified Lott dataset than the original Lott dataset, meaning that the effect of multicollinearity has been successfully reduced.

#### District of Columbia analysis

**Looking closely in District of Columbia since it had different trend compared to other states**

From Exploratory Data analysis section, we observed that District of Columbia had different trends compared to other states. We will build statistical analysis model to analyze specifically for District of Columbia and how the relationship between Right to Carry law and crime rate is like for District of Columbia.

```{r}
DONOHUE_DF_DC <- DONOHUE_DF |>
  filter(STATE=="District of Columbia")

LOTT_DF_DC <- LOTT_DF |>
  filter(STATE=="District of Columbia")
```

We made a new dataframe for both Lott and Donohue to just included District of Columbia.

We will build a multiple linear regression model using Poverty Rate, Unemployment Rate, and Right to Carry law to predict crime rate for District of Columbia

First, we will do multiple linear regression model for Donohue data.

```{r}
DC_lin_mod <- linear_reg()|>
  set_engine("lm") |>
  fit(Viol_crime_rate_1k ~ Unemployment_rate + Poverty_rate, data=DONOHUE_DF_DC)
DC_lin_mod
```

As we saw in the EDA we can see that there is high correlation between Poverty rate having estimate coefficient of 0.98326 and Violent crime rate. However, we do not see high correlation between the unemployment rate. Also, District of Columbia does not have Right to Carry Law, we can infer that Poverty_rate and Violent_crime rate has correlation. We are still unsure why there is this correlation between Poverty rate and Crime rate. 

Here is the specific model interpretation: When both poverty rate and unemployment rate is 0, the model prediction for violent crime rate is 0.834. For each unit increase in poverty rate when unemployment rate remains constant, the model predicts the violent crime rate to increase by 0.98. For each unit increase in unemployment rate, the model predicts the violent crime to decrease by 0.081.

Since Lott and Donohue contains same information about Poverty rate and unemployment, violence crime rate, the result analyzed both Lott and Donohue.

We will visualize our result.

```{r}
ggplot(DONOHUE_DF_DC, aes(x = Poverty_rate,
                y = Viol_crime_rate_1k)) +
  geom_point() +
  geom_smooth(aes(x = Poverty_rate,
                y = Viol_crime_rate_1k), method="lm") +
  labs(title="Poverty rate has high relationship with Crime rate in District of Columbia",
       y = " ",
       x = "Poverty Rate") +
  theme_minimal() +
  theme(plot.title.position = "plot")
```
From the visualization of the model, we can see more clearly there is a positive correlation between the poverty rate and the violent crime rate, which matches our previous interpretation of the model output.

```{r}
ggplot(DONOHUE_DF_DC,
       aes(x = Unemployment_rate,
           y = Viol_crime_rate_1k)) +
  geom_point() +
  geom_smooth(aes(x = Unemployment_rate,
                  y = Viol_crime_rate_1k), method = "lm", color="Orange") +
  labs(title="Unemployment rate shows some relationship with Crime rate in District of Columbia",
       y = " ",
       x = "Unemployment Rate")
```
This visualization, however, does not match the result from our model. Maybe the multiple linear regression model of predicting violence crime rate differs from just having one predictor. The above model shows the line that represents predicting violence crime rate by single predictor which is unemployment_rate. Shows a sligh relationship between the two

Here is the statistical result just using Unemployment rate to predict Violence crime rate

```{r}
lin_mod <- linear_reg()|>
  set_engine("lm") |>
  fit(Viol_crime_rate_1k ~ Unemployment_rate, data=DONOHUE_DF_DC)
lin_mod
```
Now makes sense that there is correlation between the Unemployment rate and Violence crime rate. 

Interpretation of the model: The slope (0.841) tells us when there is an increase of one unit in unemployment rate, on average, there will be 0.841 increase in violence crime rate.

### Results

Through our exploratory data analysis, we observed that there seem to be a steady increase in the population over time from 1980 to 2010, which is fairly intuitive. In comparison, there seems to be more fluctuation in the crime rates overtime. In the plot we generates, we observed an overall decreasing trend, but an increase in crime rates from around 1983 to 1991, with the peak in crime rate occurring around 1991. Next, we investigate the relationship between police staffing and violent crime rate to see how they are correlated.From the plot we can see that police presence shows a relatively steady increase over time, reaching its peak around 2000 and remain steady after since, which matches the overall decreasing trend of violent crime rate. Then, more specifically, we investigate the change in crime rate over year for each state. Despite the overall complexity in the plot we generated which is due to the number of states, we did find that District of Columbia shows the highest violence crime rate across time, whose trend matches the overall trend for all states. In comparison, Maine shows a overall relative low violent crime rate across year, and the lowest violent crime rate in 2010 which is the last year in our dataset. 

Next, we conducted more rigorous data analyses on the relationship between State, Year, RTC law and violent crime rates. Firstly, we created a panel regression model based on donohue dataset. In the model ,we specifies the model effect to be two way in order to account for the effects of both state and year and the model to be within because we are only interested in how violence in each state varied over time. Then, we applied the same model to Lott dataset and compared the estimate coefficients.Based on the coefficients comparison, we see that the predicted violence crime rate based on donuhue dataset shows that if RTC law is enacted is 0.0240 higher than if there's no RTC law. And the model output based on lott dataset demonstrates that if RTC is enacted, then the predicted violence crime rate is 0.0518 lower than if there is no RTC law, which is contradictory to the model prediction based donuhue dataset. What is noteworthy is that the demographics variables we include in each model are different, which may account for the discrepancy.

To then digged deeper into single state to answer how the relationship differ between before and after the Right to Carry law was enacted. We chose a state (Nevada) to analyze, in order to narrowing down our scope. And since there seems to be multicollinearity issue in the Lott dataset, we focus on the donohue dataset only. After constructing a multiple linear regression model, the output tells us that, before the Law was enacted for Nevada, the predicted violence crime rate is 35.42. And after the law was enacted, the predicted violence crime rate is decreased by 0.199898. This tells us that after the Right to Carry law for Nevada, there is decrease of violence crime rate, which is contradictory to the overall trend we observed for donohue data.

We then conducted analyses to check the multicollinearity of the two datasets and discover which regressors to be accounted for. First, we use pair plot to check the correlations between non-demographic variables, including violent crime rate, unemployment rate, poverty rate, and population. Overall, there doesn't seem to be strong correlations. Then, we used heatmaps to check the correlations between the demographic variables. Overall, we notice heavy correlations within race, and this applied to both datasets. In order to further investigate multicollinearity, we used resampling and leave-one-out-cross-validation method. As a result, we see that the lott dataset contains much higher variability around the estimate coefficient. Followingly, we check the VIF for both datasets to explore how much variability is due to collinearity. As a result, we see that the VIF values are much higher for the lott dataset than the donohue dataset, with the majority of VIF values above 10. This means that the model based on the lott dataset suffers from multicollinearity much more than that based on the donohue dataset. Therefore, we then tried to correct for the multicollinearity in the lott dataset by parameterizing the demographic variables differently. As a result, the new dataset does show overall much lower VIF values, meaning that the effect of multicollinearity has been successfully reduced.

Lastly, we focused on District of Columbia in order to reduce some complexity and answer if some state's violence crime rate is influenced by other variable (i.e., non-demographic variable). Then, we look more closely into the change in poverty rate, unemployment rate, and violent crime rate across time in District of Columbia. Based on the visualization we did earlier, we saw that while unemployment rate remain relatively steady overtime, poverty rate shows a similar trend as violent crime rate, showing an sharp increase around 1990s. This suggests a stronger correlation between poverty rate and violent crime rate. To test this finding, we conducted a multiple regression model, including both poverty rate and unemployment rate as predictors. The model output matches our expectation, with the estimate coefficient for poverty rate being 0.98 and that for unemployment rate being only -0.08. However, the visualization for violent crime rate against unemployment rate shows a positive correlation, despite the higher variability, which does not fit the model estimate. Therefore, we constructed another single linear regression model that includes only unemployment rate as predictor, and the output does show a positive coefficient estimate. Further research should be done to account for the discrepancy we have found.


## Conclusion

Understanding what accounts for violent crime rate is crucial for the benefits of our society. However, there are enormous factors that come in to play, and different states may have different situations. In this study, we mainly focused on the effects of Right to Carry law, unemployment rate, as well as poverty rate on the change in violent crime rate. Moreover, we narrowed down our scope on mainly two states, which are District of Columbia and Nevada. Overall, we confidently found a strong correlation between poverty rate and violent crime rate. However, unemployment rate shows a weaker predictive power, and the result of which could be impacted by the type of model we used. Similarly, the effect of RTC law on violent crimes is open to discussion and differ across states. As we saw the discrepancy of the results in Donohue and Lott studies may be due to multicollinearity, we also noticed that in Nevada, there is a negative relationship between RTC law and violent crime rate for the state of Nevada, but the relationship is positive when taking all states into account. Putting all these results together, it is clear that the relationship between RTC law and violent crimes rate is very complicated, and no definitive conclusion can be made confidently.
